{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b47f96a0-9671-462d-94bd-bee39079a0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "from PIL import ImageOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "672d01ec-13f8-4984-9564-c83b4d21a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = {0: 'あ - a',\n",
    " 1: 'い - i',\n",
    " 2: 'う - u',\n",
    " 3: 'え - e',\n",
    " 4: 'お - o',\n",
    " 5: 'か - ka',\n",
    " 6: 'き - ki',\n",
    " 7: 'く - ku',\n",
    " 8: 'け - ke',\n",
    " 9: 'こ - ko',\n",
    " 10: 'さ - sa',\n",
    " 11: 'し - shi',\n",
    " 12: 'す - su',\n",
    " 13: 'せ - se',\n",
    " 14: 'そ - so',\n",
    " 15: 'た - ta',\n",
    " 16: 'ち - chi',\n",
    " 17: 'つ - tsu',\n",
    " 18: 'て - te',\n",
    " 19: 'と - to',\n",
    " 20: 'な - na',\n",
    " 21: 'に - ni',\n",
    " 22: 'ぬ - nu',\n",
    " 23: 'ね - ne',\n",
    " 24: 'の - no',\n",
    " 25: 'は - ha',\n",
    " 26: 'ひ - hi',\n",
    " 27: 'ふ - fu',\n",
    " 28: 'へ - he',\n",
    " 29: 'ほ - ho',\n",
    " 30: 'ま - ma',\n",
    " 31: 'み - mi',\n",
    " 32: 'む - mu',\n",
    " 33: 'め - me',\n",
    " 34: 'も - mo',\n",
    " 35: 'や - ya',\n",
    " 36: 'ゆ - yu',\n",
    " 37: 'よ - yo',\n",
    " 38: 'ら - ra',\n",
    " 39: 'り - ri',\n",
    " 40: 'る - ru',\n",
    " 41: 'れ - re',\n",
    " 42: 'ろ - ro',\n",
    " 43: 'わ - wa',\n",
    " 44: 'ゐ - wi',\n",
    " 45: 'ゑ - we',\n",
    " 46: 'を - wo',\n",
    " 47: 'ん - n',\n",
    " 48: 'ゝ'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "629bd921-83d9-4a7e-b0fd-8d90064ba95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: る - ru 40\n",
      "Class 0: 0.00%\n",
      "Class 1: 0.00%\n",
      "Class 2: 0.00%\n",
      "Class 3: 0.00%\n",
      "Class 4: 0.00%\n",
      "Class 5: 0.00%\n",
      "Class 6: 0.00%\n",
      "Class 7: 0.00%\n",
      "Class 8: 0.00%\n",
      "Class 9: 0.00%\n",
      "Class 10: 0.00%\n",
      "Class 11: 0.00%\n",
      "Class 12: 0.00%\n",
      "Class 13: 0.00%\n",
      "Class 14: 0.00%\n",
      "Class 15: 0.01%\n",
      "Class 16: 0.00%\n",
      "Class 17: 0.00%\n",
      "Class 18: 0.00%\n",
      "Class 19: 0.00%\n",
      "Class 20: 0.37%\n",
      "Class 21: 0.00%\n",
      "Class 22: 0.00%\n",
      "Class 23: 0.00%\n",
      "Class 24: 0.00%\n",
      "Class 25: 0.00%\n",
      "Class 26: 0.00%\n",
      "Class 27: 0.00%\n",
      "Class 28: 0.00%\n",
      "Class 29: 0.00%\n",
      "Class 30: 0.00%\n",
      "Class 31: 0.00%\n",
      "Class 32: 0.00%\n",
      "Class 33: 0.00%\n",
      "Class 34: 0.00%\n",
      "Class 35: 0.00%\n",
      "Class 36: 0.00%\n",
      "Class 37: 0.00%\n",
      "Class 38: 0.00%\n",
      "Class 39: 0.00%\n",
      "Class 40: 99.59%\n",
      "Class 41: 0.00%\n",
      "Class 42: 0.03%\n",
      "Class 43: 0.00%\n",
      "Class 44: 0.00%\n",
      "Class 45: 0.00%\n",
      "Class 46: 0.00%\n",
      "Class 47: 0.00%\n",
      "Class 48: 0.00%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAN6UlEQVR4nO3cS4jW9dvH8WsOjokpSSWISWlSkqRZFO20LIqgVkJFbYPAVYItskWIdNqURFLUogNKyyIkogORCJ0WoWlFUmJSLTyNptPoHP6LP1w9PJtnru+Dv+7G12vth/ue29t591t09U1OTk4GAERE/z/9BgDoHaIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAaXCqf7Cvr+98vo9/ja4+B/9P4d+6+sx7/Tve8p1o2bR8Di0b3/G/dfVZTOV1PCkAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACBN+SAe/Jt0daCtyyN6vXwY0HG7/5oOn4MnBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoApAv6IF6Xx8yqevm9TVctn/nAwEB5M3fu3PKm1fDwcHkzMTFxHt4J/xaeFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgHRBX0ltMTk52cnrDA62/dW0vL+ufqahoaGm3axZs8qbSy+9tLxZunRpebNixYry5tZbby1vIiLGx8fLm9dee6282bVrV3kzOjpa3rjG2ps8KQCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIDmIV9TfX+9oy+aOO+4obyIi5s2bV978+uuv5c2iRYvKm2XLlpU3ERHXXnttebN48eLyZmBgoLw5duxYeXPNNdeUNxERV155ZXkzf/788uaRRx4pbw4cOFDe0Js8KQCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIDmI14HJycnyZmxsrOm1NmzYUN5cccUV5U3Lz3TixInyJiJiz5495c2OHTs6eZ3rrruuvNm0aVN5E9F2uPCNN94ob3777bfyhunDkwIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAFLf5BQvm/X19Z3v99K5/v56E1sOwbV8dkNDQ+VNRMTatWvLm7vuuqu8aTke98UXX5Q3ERG///57eXP69Ony5oYbbihvtm7dWt4sWbKkvImI2LJlS3nz+uuvlzcjIyPlTYuJiYlOXoe/TeX3lycFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgCkC/ogXldaDu+1bFrNnDmzvBkdHS1vWo4JRrR996666qry5vnnny9vVq9eXd68/PLL5U1ExAsvvFDeDA8PN71WVcvfbev3gXYO4gFQIgoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiD//QbuBBMTEyUN11ekBwZGenstVrMmzevvFm/fn15s2bNmvJm+/bt5c0rr7xS3kS0XTx1vXT6Ol+Xqz0pAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgOYjXo1qPkrUcyWrZtLy/WbNmlTcREQ8//HB588ADD5Q3H374YXmzdevW8uaPP/4obyK6O253vg6t/W8O7/UmTwoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEgO4tFkcLD+1bnzzjubXmv9+vXlzd69e8ub5557rrz55ZdfyptWXR2q48LmSQGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAMlBPGJycrK8WbFiRXmzcePG8iYi4sSJE+XN008/Xd7s27evvGn57Fo20BVPCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQJryldS+vr7z+T6mtZarmP39bb1u+XtauHBhedNy8XTu3LnlTUTEk08+Wd58+eWX5c34+Hh50+tavg8t39eW12n9nTIxMdHJa/X6Ndvz9f48KQCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIE35IF6vH4dq0dWRvy6PCc6ZM6e8efTRR8ubVatWlTdbtmwpbyIiPv744/Lm7NmzTa9V1fJ3O3PmzKbXGhyc8j/X1HJYseWzGxsbK29aDxB2eXyvK730+9WTAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAUv3C1gWuq8NarUfTHnzwwfJm3bp15c2rr75a3rz33nvlTUTbZ7506dLyZsmSJeXNsmXLypsFCxaUNxFtxw6HhobKm9OnT5c3+/fvL28+++yz8iYi4ueffy5vWg72tXzveumwXStPCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASH2TU7zg1NUhuC61/Ewtm4GBgfLmnnvuKW8iIp599tny5v333y9vtm/fXt60HJyLiLj99tvLm+uvv768GR8fL29ajrMdOHCgvImIOHnyZHnTckSv5cjfTTfdVN6Mjo6WNxERzzzzTHmzc+fO8ubcuXPlTa8fxJvK+/OkAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGANOWDeP393fSj9aBUVwf7Wj6HVatWlTfbtm0rbyIihoeHy5uvvvqqvLn55pvLmwULFpQ3ERE//PBDedNyAO3rr78ubw4fPlzejIyMlDcRbQf7Wlx88cXlzY033ljevPjii+VNRMTY2Fh589BDD5U3P/30U3kzMTFR3kR0d0jPQTwASkQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBp8J9+A/+klouny5cvL282b95c3qxcubK8iYg4cuRIeXP11VeXN59//nl5s3Xr1vImIuKbb74pb44ePVretFyq7Oq6ZZevderUqfJm9+7d5c2OHTvKm4iIp556qrxpuVR84MCB8mY68KQAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYA05YN4XR7+atHX11fezJ07t7zZuHFjeXP33XeXN2NjY+VNRNsRr23btpU3n3zySXlz/Pjx8iai9797VV3+PC3/Llre3/j4eHlz8ODB8iai7f0tXLiwvGn57KYDTwoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEhTPojXlS6PUA0NDZU3LYe1/vzzz/LmzTffLG8iIl566aXy5tChQ+XNuXPnyptWvXyYrNeP2/Wyyy67rGk3Y8aM8ubUqVPlzXQ7xDhVnhQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJB67iBe6xGqlt3w8HB5884775Q3O3fuLG/efvvt8iYi4siRI027qpbjbAMDA02vNXv27PKm5Whay5G/06dPlzdjY2PlTUTbZ97y76LldebMmVPerF27tryJiDh27Fh5s2/fvvJmYmKivJkOPCkAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACD13EG8Lo2OjpY3b731VnnTcmDsr7/+Km9aX6vF4sWLy5v777+/6bWWL19e3rQcaDtz5kx5s2fPnvLmgw8+KG8iIvbv31/enD17tryZOXNmebNu3bryZs2aNeVNRMS7775b3nz//fdNr3Uh8qQAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgCkvsnJyckp/cGOrm92yc/0X4OD9WO5mzdvLm9uu+228iai7SrmjBkzypvVq1eXN7fcckt5s3fv3vImImLTpk3lzY8//lje3HfffeXN448/Xt4cPHiwvImIeOyxx8qb7777rryZmJgob6b46/QfM5X350kBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgCpfgmNntZykGtgYKC8mT17dnlz8uTJ8iYiYteuXeXN4cOHy5tPP/20vNmwYUN5c++995Y3ERFPPPFEeXP06NHyZuXKleXNRx99VN5s27atvIno7rjdhcqTAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAUt/kFC+o9fX1ne/38v96nZZDcF1p+Zlaf56W1+rvr/+3wapVq8qb9evXlzcREYsWLSpvRkZGml6rav78+Z1sIiIOHTpU3nz77bflTctxu927d5c3w8PD5U1Ebx+3a/391dXviKl8dp4UAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQeu4gXq/r6nPo8sBfy0G8ls/hkksuKW8iIhYsWFDeXH755eXNjBkzypuzZ8+WN0eOHClvItoOyB0/fry8OXPmTHnTovU7Pt2OX7buHMQD4LwTBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoApClfSW25pNmi9QJiV1cGW3T53lpeq6v31+Wl3V6+pNmlXv530aqX319XvydbjY+P/59/prd/AgA6JQoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAGlwqn+w14+mdXVsreVn6vXPoasDY10eMuvqc+j1w4D8O8yaNau8ueiii87DO/GkAMD/IAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAKlvsssrZQD0NE8KACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKAKT/AKHG71In2t4YAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the transformation for preprocessing the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "class MobileNet(nn.Module):\n",
    "    def __init__(self, num_classes=49):\n",
    "        # https://www.researchgate.net/publication/369777559_A_Robust_Residual_Shrinkage_Balanced_Network_for_Image_Recognition_from_Japanese_Historical_Documents\n",
    "        super(MobileNet, self).__init__()\n",
    "        self.mobilenet = models.mobilenet_v3_large(weights=None)\n",
    "        # self.mobilenet = models.mobilenet_v3_large(weights=\"MobileNet_V3_Large_Weights.DEFAULT\")\n",
    "        self.mobilenet.features[0][0] = nn.Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        self.mobilenet.classifier[3] = nn.Linear(1280, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mobilenet(x)\n",
    "\n",
    "# Load your trained MobileNet model\n",
    "model = torch.load('mobilenet-hiragana.pth')  # Update the path to your saved model\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Load and preprocess the image\n",
    "image = Image.open('test.jpg')  # Update the path to your test image\n",
    "image = ImageOps.invert(image)\n",
    "input_tensor = transform(image)\n",
    "input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "plt.imshow(input_tensor.permute(1, 2, 0), cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "#plt.title(dataset.classes[label])\n",
    "\n",
    "# If you're using GPU, move the input batch to GPU\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "\n",
    "# Convert the output probabilities to predicted class index\n",
    "probabilities = torch.softmax(output, dim=1)\n",
    "predicted_class_index = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "# Get the label for the predicted class index\n",
    "predicted_label = class_labels[predicted_class_index]\n",
    "\n",
    "print(\"Predicted label:\", predicted_label, predicted_class_index)\n",
    "\n",
    "class_probabilities = probabilities.squeeze().tolist()\n",
    "\n",
    "# Find the number of classes\n",
    "num_classes = len(class_probabilities)\n",
    "\n",
    "# Create a dictionary to store class counts\n",
    "class_counts = {}\n",
    "\n",
    "# Count the number of instances for each class\n",
    "for i, prob in enumerate(class_probabilities):\n",
    "    class_counts[i] = prob\n",
    "\n",
    "# Calculate the total sum of probabilities\n",
    "total_probability = sum(class_counts.values())\n",
    "\n",
    "# Calculate the percentage of each class\n",
    "class_percentages = {class_idx: (count / total_probability) * 100 for class_idx, count in class_counts.items()}\n",
    "\n",
    "# Print the percentage of all classes\n",
    "for class_idx, percentage in class_percentages.items():\n",
    "    print(f\"Class {class_idx}: {percentage:.2f}%\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
